{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "971e46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Any\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3355d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is c:\\Users\\LENOVO\\OneDrive\\Desktop\\MS DS Courses\\MS DSP 453\\Project Code/full_contract_txt\n",
      "\n",
      "There are 510 txt files in the directory\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.getcwd() + \"/full_contract_txt\"\n",
    "\n",
    "print(f'The current working directory is {dir_path}\\n')\n",
    "\n",
    "files_list = os.listdir(dir_path)\n",
    "\n",
    "print(f'There are {len(files_list)} txt files in the directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec61e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data from the files is loaded\n"
     ]
    }
   ],
   "source": [
    "for files in files_list:\n",
    "    if files.endswith('.txt'):\n",
    "        loader = TextLoader(os.path.join(dir_path, files),encoding='utf8')\n",
    "        data = loader.load()\n",
    "        \n",
    "print(f'All the data from the files is loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c9744db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Split 1 documents into 15 chunks.\n",
      "[INFO] Generating embeddings for 15 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embeddings shape: (15, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingPipeline:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(f\"[INFO] Loaded embedding model: {model_name}\")\n",
    "\n",
    "    def chunk_documents(self, documents: List[Any]) -> List[Any]:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        print(f\"[INFO] Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "\n",
    "    def embed_chunks(self, chunks: List[Any]) -> np.ndarray:\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        print(f\"[INFO] Generating embeddings for {len(texts)} chunks...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"[INFO] Embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    emb_pipe = EmbeddingPipeline()\n",
    "    chunks = emb_pipe.chunk_documents(data)\n",
    "    embeddings = emb_pipe.embed_chunks(chunks)\n",
    "    # print(\"[INFO] Example embedding:\", embeddings[0] if len(embeddings) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded embedding model: all-MiniLM-L6-v2\n",
      "[INFO] Split 1 documents into 15 chunks.\n",
      "[INFO] Generating embeddings for 15 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embeddings shape: (15, 384)\n",
      "[INFO] Added 15 documents to ChromaDB collection.\n",
      "{'ids': [['contract_5', 'contract_9']], 'embeddings': None, 'documents': [['whenever necessary, which shall be executed by signing supplemental agreements upon consensus of both Parties.   (c) Party B shall ensure the vehicles are in good conditions, the compartments are properly sealed without leakage and the vehicles are equipped with fire-fighting equipment. In the event of parcel damage resulting from leakage or fire, Party B shall indemnify at the standard rate of RMB200 per parcel, and indemnify the actual price for high-end insured parcel (or indemnify by the value of the parcel provided by arbitration department determined by Party A).   (d) Party B shall have valid and legal licenses for national road transportation. In the event of loss caused to Party A by delivery delay due to vehicles detention for the lack of license, Party B shall compensate for any loss to Party A.   (e) Party B shall arrive at the network partners determined by Party A according to the time and route stipulated in this Agreement, and strictly comply with the start time and', \"licenses to Party A, and guarantee the authenticity, completeness, legality and validity of such licenses and materials. Party B's drivers shall have at least two years driving experience in large trucks and have relevant licenses. Party B shall bear any consequences and legal liability arising out of Party B's non-compliance, and Party A shall have no liability.   (h) Party B shall bear any consequences and economic punishments arising out of the breach of traffic rules by Party B's drivers and other staff, and Party A shall have no liability.   (i) Party B shall be responsible for driving safety. Party B shall bear any legal liability arising out of severe traffic accidents causing vehicle damage and personnel casualties, and Party A shall have no liability. Party B shall be responsible for any damages resulting from severe accidents causing Party A's personnel casualties. Party A has the right to indemnify from Party B upon advance payment of damages.   6. Party B shall purchase\"]], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'source': 'contract_5'}, {'source': 'contract_9'}]], 'distances': [[1.6665430068969727, 1.6861357688903809]]}\n"
     ]
    }
   ],
   "source": [
    "class vectorDB:\n",
    "    def __init__(self,embeddings,chunks,db_name):\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = chunks\n",
    "        self.client = chromadb.Client()\n",
    "        if db_name in [col.name for col in self.client.list_collections()]:\n",
    "            self.collection = self.client.get_collection(db_name)\n",
    "        else:\n",
    "            self.collection = self.client.create_collection(db_name)\n",
    "\n",
    "    def Loading_data_to_ChromaDB(self):\n",
    "    # Add embeddings and documents to the collection\n",
    "        self.collection.add(\n",
    "            embeddings=self.embeddings,\n",
    "            documents=[chunk.page_content for chunk in self.chunks],\n",
    "            metadatas=[{\"source\": f\"contract_{i}\"} for i in range(len(self.chunks))],\n",
    "            ids=[f\"contract_{i}\" for i in range(len(self.chunks))]\n",
    "        )\n",
    "        print(f\"[INFO] Added {len(self.chunks)} documents to ChromaDB collection.\")\n",
    "\n",
    "    def queryDB(self,query):\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=2\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    emb_pipe = EmbeddingPipeline()\n",
    "    chunks = emb_pipe.chunk_documents(data)\n",
    "    embeddings = emb_pipe.embed_chunks(chunks)\n",
    "\n",
    "    vdb = vectorDB(embeddings,chunks,'contract')\n",
    "    vdb.Loading_data_to_ChromaDB()\n",
    "    res = vdb.queryDB(\"DEFINITION OF CONFIDENTIAL INFORMATION.\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd16105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Groq_model(query):\n",
    "    groq_api_key = \"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\"\n",
    "    model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "    Groq_model = ChatGroq(groq_api_key=groq_api_key, model_name=model_name)\n",
    "    print(f\"[INFO] Groq LLM initialized: {Groq_model}\")\n",
    "    res = vdb.queryDB(query)\n",
    "\n",
    "    text = [r for r in res['documents']]\n",
    "    context = [\"\\n\\n\".join(i) for i in text]\n",
    "\n",
    "    prompt = f\"\"\"Summarize the following context for the query: '{query}'\\n\\nContext:\\n{context}\\n\\nSummary:\"\"\"\n",
    "    response = Groq_model.predict(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Groq LLM initialized: client=<groq.resources.chat.completions.Completions object at 0x000002352C594C50> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002352C595650> model_name='openai/gpt-oss-120b' model_kwargs={} groq_api_key=SecretStr('**********')\n",
      "[INFO] Groq LLM initialized: client=<groq.resources.chat.completions.Completions object at 0x000002352C595E50> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002352C596950> model_name='openai/gpt-oss-120b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(\"Summary File.txt\", \"w\" ,encoding = 'UTF-8') as f:\n",
    "    f.write(Groq_model(\"DEFINITION OF CONFIDENTIAL INFORMATION.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a604bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project Code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
